{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GATHERING AND GETTING TO KNOW THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires three different datasets.\n",
    "\n",
    "First was the enhanced tweets archive of the 'we rate dogs' twitter page that was sent to the instructor and provided as a csv file in the classroom (**\"twitter_archive.csv\"**). it contains 2356 rows and 17 columns. 7 of these columns were extracted from the text column which is why it referred to as 'enhanced'. This dataset contains information on each tweet as regard the id, text, source, url timestamp and a reply and retweet column. It was downloaded directly from the classroom, uploaded to the alx inbuilt jupyter notebook and loaded into a dataframe which ws named twitter_raw.\n",
    "\n",
    "The second dataset was an image prediction tsv(tab separated variables) file that was provided by the project instructor. it contains prediction about the breed of the dog present in each tweet as regard their images based on prediction which has already been done behind the scenes. It contained 3 predictions, the respective confidence level and a boolean column that tells whether each prediction is a dog or not. It contains 2075 rows. It was downloaded programmatically through a link provided in the classroom also and saved into a dataframe called **images_raw**.\n",
    "\n",
    "The third dataset was additional information about each tweet scraped from the twitter api in json format saved and was saved as a txt file provided by the project instructor. The retweet count and likes count for each tweet was then extracted from the text file and saved to a dataframe named **text_raw**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSESSING THE DATASETS\n",
    "All the three dataframes were assessed visually and programmatically to understand and to look out for quality and tidiness issues, the quality and tidiness issues found are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality issues\n",
    "These quality issues were gotten as a result of checking the datasets both programmatically and visually. some of the codes used were pandas sample(), describe(), info(), .isnull()/isna() and many more.\n",
    "\n",
    "1. Timestamp is in object format.\n",
    "\n",
    "2. Rating denominators has values other than 10.\n",
    "\n",
    "3. Retweeted tweets to be dropped.\n",
    "\n",
    "4. Creating another column named date extracted from the Timestamp column.\n",
    "\n",
    "5. Dogs with unreasonable names like 'a' to be replaced with unknown.\n",
    "\n",
    "6. Dogs with names as 'None' be replaced with unknown.\n",
    "\n",
    "7. Dog stages contains null values(None).\n",
    "\n",
    "8. image file columns not descriptive enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness issues\n",
    "Some of the codes used are pandas list, .dtypes, .head() few times.\n",
    "\n",
    "1. Dropping irrelevant columns.\n",
    "\n",
    "2. Dog stages are represented with multiple columns.\n",
    "\n",
    "3. P2, P2_conf, P2_dog,P3, P3_conf, P3_dog columns may not be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANING\n",
    "  Copies of all three dataframes were made in other to keep the original data  intactand then cleaning was performed on the copy of the dataframes.\n",
    "  All quality issues discovered were solved first using the define-code-test approach according to the template given. some of the functions used includes pd.to_datetime(), str.replace() and rename() etc.\n",
    "  Tidiness issues were then solved using functions like df.drop(), + operator to conctenate columns and merge() to merge all three tables together. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL RESULT\n",
    "This conludes my data wrangling journey and finally I was able to clean my original dataset and was left with a master dataset holding all the needed and required information about each tweet from basic tweet information to dog breed prediction.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
